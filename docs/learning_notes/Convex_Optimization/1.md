
> date: 2023-01-08

## 一、数学优化

$\textbf{Definition.}$ 形如
$$
\begin{aligned}
\min_{x} \quad & f_0(x) \\
\text{s.t.} \quad & f_i(x) \leq b_i, \quad i=1, \cdots, m_1, \\
  & f_i(x) = b_j, \quad i = m_1+1, \cdots, m_2,
\end{aligned}
$$
我们称之为 **数学优化问题（Mathematical Optimization Problem）**。其中，$x \in \mathbb R^n$，我们称之为 **优化变量（optimization variable）**；函数 $f_0: \mathbb R^n \mapsto \mathbb R$，我们称之为 **目标函数（objective function）**；函数 $f_i: \mathbb R^n \mapsto \mathbb R$，我们称之为 **约束函数（constraint function）**。如果目标函数与约束函数均为线性，即对任意的 $x, y \in \mathbb R^n, \ \alpha, \beta \in \mathbb R$，都有
$$
f_i(\alpha x + \beta y) = \alpha f_i(x) + \beta f_i(y),
$$
则我们称对应的优化问题为 **线性优化（linear program）**。如果一个优化问题不是线性规划，则我们称其为 **非线性优化（nonlinear program）**。类似地，如果目标函数与约束函数均为凸函数，即对任意的 $x, y \in \mathbb R^n$，任意 $\alpha, \beta \in \mathbb R$ 且满足 $\alpha + \beta = 1, \ \alpha,\beta \geq 0$，都有
$$
f_i(\alpha x + \beta y) \leq \alpha f_i(x) + \beta f_i(y),
$$
则我们称对应的优化问题为 **凸优化问题（convex optimization problems）**。不难发现凸优化问题相较线性优化是更广义的，因为在条件中我们用小于等于替换掉了等号。

## 二、最小二乘与线性规划

这里我们简单介绍两类极为常见的隶属于凸优化问题的子问题：最小二乘与线性规划，同时说明关于凸优化的一些重要认知。

### (1) 最小二乘

$\textbf{Definition.}$ 形如
$$
\begin{aligned}
\min_{x} \quad & f_0(x) = \| Ax-b \|_2^2 = \sum_{i=1}^k (a_i^Tx - b_i)^2,
\end{aligned}
$$
其中 $A\in \mathbb R^{k\times n}, k \geq n$；$a_i$ 是矩阵 $A$ 的行向量，我们称之为 **最小二乘问题（least-squares problem）**。

求解最小二乘问题可以被简化为求解一系列线性方程：
$$
(A^TA)x = A^Tb,
$$
因此我们可以得到解析解 $x = (A^TA)^{-1}A^Tb$。求解上述问题的复杂度应为 $\mathcal O(n^2k)$。



### (2) 线性规划

我们上面已经介绍过线性优化，也常称该类问题为线性规划。该类问题具体形式为
$$
\begin{aligned}
\min_{x} \quad & c^Tx \\
\text{s.t.} \quad & a_i^Tx \leq b_i, \quad i=1, \cdots, m_1, \\
  & a_i^Tx = b_j, \quad i = m_1+1, \cdots, m_2,
\end{aligned}
$$
其中 $c, a_1, \cdots, a_{m_2}\in \mathbb R^n, \ b_1, \cdots, b_{m_2} \in \mathbb R$。

线性规划问题没有像最小二乘问题一样的解析解。实践中，求解线性规划的复杂度通常为 $\mathcal O(n^2m)$ ，假设 $m \geq n$。

有一些常用的技巧可以将部分问题转化为线性规划问题，最为典型的例子是LASSO回归。



### (3) 凸优化

等式约束 $f(x) = b$ 可以被理解为两个不等式约束，即 $f(x) \leq b$ 与 $f(x) \geq b$。为了使这两个不等式约束的可行域都是凸集，一般我们需要要求函数 $f(x)$ 是线性的。因此，在我们后续的所有讨论中，我们设定凸优化问题的等式约束均为线性约束，即形如 $c^Tx = b$。

我们上面已经看到，最小二乘问题与线性规划问题都属于凸优化问题的特殊情况。凸优化问题同样没有解析解。实践中，识别出一个问题是凸优化问题往往很难。

凸优化问题最为重要的性质：所有的局部最优解都是全局最优解。

<br>

Why are we so obsessed about convexity?

- For unconstrained convex optimization, all local minima are global minima. 
- And it is easy to “descend” computationally (go against the gradient).
- Newton’s method: the Newton direction is ALWAYS a descent direction – no need to regularize (for THIS reason).
- For constrained convex optimization, there is a reasonably easy test, whose cost is (roughly) the cost of solving a linear program, but it is most likely an outcome of any algorithm for solving the problem. 
- The test: “are the optimality conditions feasible” – KKT conditions.
- Many convex optimization problems are guaranteed to show good theoretical complexity and many also show good complexity empirically.
